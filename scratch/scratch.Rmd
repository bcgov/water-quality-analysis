---
title: "Trend Exploration"
output: html_notebook
---

```{r setup, include=FALSE}
library(canwqdata)
library(dplyr)
library(ggplot2)
library(lubridate)
library(wqbc)
library(mgcv)
library(broom)
library(readr)
library(schoenberg) # devtools::install_github('gavinsimpson/schoenberg')
library(here)
library(feather)
library(purrr)
source(here::here("R/functions.R"))
dir.create(here::here("out"))
```


```{r load, eval=FALSE}
bc_stations <- wq_sites() %>% 
  filter(PROV_TERR == "BC")

bc_data <- wq_site_data(unique(bc_stations$SITE_NO)) %>% 
  left_join(select(bc_stations, SITE_NO, PEARSEDA), 
            by = "SITE_NO")

bc_data$VALUE_VALEUR[bc_data$VALUE_VALEUR < -999] <- NA

write_feather(bc_data, here::here("data/bcdata.feather"))
```


```{r EC-data}
# load parameters:
bc_data <- read_feather(here::here("data/bcdata.feather"))
params <- read_csv(here::here("data/parameters.csv"))

bc_tidy <- tidy_ec_data(bc_data, cols = "PEARSEDA") %>% 
  add_shortnames() %>% 
  filter(PARAM_SHORT_NAME %in% params$Variable_short_name, 
         year(DateTime) >= 2003) %>% 
  group_by(SITE_NO, PARAM_SHORT_NAME, DateTime, Units, DetectionLimit, 
           ResultLetter, PEARSEDA) %>% 
  summarise(Value = mean(Value, na.rm = TRUE), 
            n = n()) %>% 
  ungroup() %>% 
  mutate(year = year(DateTime), 
         month = month(DateTime), 
         censored = make_censor(ResultLetter))

bc_site_var_summary <- bc_tidy %>% group_by(SITE_NO, PARAM_SHORT_NAME) %>% 
  summarize(min_year = min(year, na.rm = TRUE), 
            max_year = max(year, na.rm = TRUE), 
            nyears = max_year - min_year,
            n_obs = n(),
            n_cens = sum(censored != "none")) %>% 
  filter(max_year >= 2013, nyears >= 10) %>% 
  ungroup()

bc_tidy <- semi_join(bc_tidy, bc_site_var_summary, 
                     by = c("SITE_NO", "PARAM_SHORT_NAME"))

fraser <- filter(bc_tidy, PEARSEDA == "FRASER-LOWER MAINLAND")


```

```{r}
plot_station_vars(fraser, "BC08MF0001")
sites <- unique(bc_tidy$SITE_NO)
var_plots <- map(sites, ~ plot_station_vars(bc_tidy, .x)) %>% 
  set_names(sites)


iwalk(var_plots, ~ ggsave(file.path(here::here("out"), paste0(.y, ".pdf")), 
                          .x, width = 10.5, height = 8, units = "in"))
```


```{r}
# foo <- standardize_wqdata(fraser_tidy)

# Try a trend test with Fraser River at Hope
fraser_hope_arsenic <- filter(fraser, 
                              SITE_NO == "BC08MF0001",
                              PARAM_SHORT_NAME == "ARSENIC TOTAL") %>% 
  ungroup() %>% 
  clean_wqdata(sds = 5, delete_outliers = TRUE)

## Only keep data from 2003 on, Aggregate by month, encode a numeric 'Time' 
# variable from Date
# fraser_hope_arsenic <- fraser_hope_arsenic %>% 
#   mutate(month = month(DateTime, label = FALSE), 
#          year = year(DateTime)) %>% 
#   filter(year >= 2003) %>% 
#   group_by(PARAM_SHORT_NAME, month, year, censored) %>% 
#   summarise(month_avg = mean(avg_val, na.rm = TRUE)) %>% 
#   mutate(Date = as.Date(paste(year, month, "15", sep = "-")))


# Make it into a complete time series:
# month_df <- data.frame(Date = seq(min(fraser_hope_arsenic$Date), 
#                                   max(fraser_hope_arsenic$Date), 
#                                   by = "month"))

fraser_hope_arsenic <- fraser_hope_arsenic %>% #right_join(fraser_hope_arsenic, month_df) %>% 
  ungroup() %>% 
  mutate(censored = make_censor(ResultLetter), 
         month = month(Date, label = FALSE), 
         year = year(Date), 
         Time = as.numeric(Date) / 1000) %>% 
  arrange(Date)

ggplot(fraser_hope_arsenic, aes(x = Date, y = Value)) + 
  geom_point(aes(shape = censored, colour = Outlier)) + 
  geom_line() +
  geom_smooth(method = "lm")
```

Fit a gam with a cyclic cubic spline smooth on month, which guarantees that the
end of December and beginning of January match up (cyclic), and a smooth trend:
```{r}
fraser_arsenic_gam <- mgcv::gam(Value ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, bs = "cr", k = 20), 
                                 # correlation = corCAR1(form = ~ 1 | year),
                                 data = fraser_hope_arsenic, 
                                 family = Gamma,
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam, main = "ACF"))
pacf(resid(fraser_arsenic_gam, main = "pACF"))
# These look fine (I think) so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam
summary(fraser_arsenic_gam)
gam.check(fraser_arsenic_gam)
draw(fraser_arsenic_gam)
# not great
```

Those checks on the model fit aren't great - try logging the response variable
(could also try with a different family, probably Gamma)

```{r}
fraser_hope_arsenic$log_avg <- log(fraser_hope_arsenic$Value)

fraser_arsenic_gam_log <- mgcv::gamm(log_avg ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, k = 20),
                                   correlation = corCAR1(),
                                 data = fraser_hope_arsenic, 
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam_log$lme), main = "ACF")
pacf(resid(fraser_arsenic_gam_log$lme), main = "pACF")
# These look fine so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam_log
summary(fraser_arsenic_gam_log$gam)
gam.check(fraser_arsenic_gam_log$gam)
draw(fraser_arsenic_gam_log$gam)

```

Now let's try to plot the trend:

```{r}

# Generate a data frame to hold the predicted values
pred_data <- tibble(Date = seq(min(fraser_hope_arsenic$Date, na.rm = TRUE), 
                               max(fraser_hope_arsenic$Date, na.rm = TRUE), 
                               length.out = 200), 
                    Time = as.numeric(Date) / 1000, 
                    month = month(Date))

## predict trend contributions
p  <- predict(fraser_arsenic_gam_log$gam, newdata = pred_data, type = "terms", 
              se.fit = TRUE)


## combine with the predictions data, including fitted and SEs
pred_data <- mutate(pred_data, 
                    pred = p$fit[,2] + coef(fraser_arsenic_gam_log$gam)["(Intercept)"],  
                    se = p$se.fit[,2])

# Plot - note need to exp() the predicted values because they were logged
ggplot(pred_data, aes(x = Date, y = exp(pred))) +
  geom_point(data = fraser_hope_arsenic, aes(y = Value)) + 
  geom_ribbon(aes(ymin = exp(pred - se), ymax = exp(pred + se)), fill = "grey") + 
  geom_line()

```


Testing out with brms, which opens up more distributions and allows modelling 
of censored data, as well as distributional modelling of input parameters:
```{r}
library(brms)
library(bayesplot)
options(mc.cores = 4L)

# foo <- brm(bf(Value ~ s(month, bs = "cc", k = 12) + 
#          s(Time, k = 20)),
#      data = fraser_hope_arsenic, 
#      family = Gamma(link = "log")) # log link to ensure not negative
# 
# pairs(foo, off_diag_args = list(size = 0.1))
# pp_check(foo)
# pp_check(foo, type = "ecdf_overlay")

foo <- brm(bf(Value| cens(censored) ~ s(month, bs = "cc", k = 12) +
                s(Time, k = 20), 
              family = gen_extreme_value(link = "log")),
           data = fraser_hope_arsenic, 
           # family = lognormal(), 
           control = list(adapt_delta = 0.99))

pp_check(foo)
pp_check(foo, type = "ecdf_overlay")
summary(foo)
pairs(foo, off_diag_args = list(size = 0.1))
plot(foo, N = 2, ask = FALSE)
plot(marginal_effects(foo), ask = FALSE, points = TRUE)
```


