---
title: "Trend Exploration"
output: html_notebook
---

```{r setup, include=FALSE}
library(canwqdata)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(wqbc)
library(mgcv)
library(broom)
library(readr)
library(schoenberg) # devtools::install_github('gavinsimpson/schoenberg')
library(here)
library(feather)
library(purrr)
source(here::here("R/functions.R"))
dir.create(here::here("out"))
```


```{r load, eval=FALSE}
bc_stations <- wq_sites() %>% 
  filter(SITE_NO %in% wqbc::ems_ec_stations$envirodat_no)

bc_data <- wq_site_data(unique(bc_stations$SITE_NO)) %>% 
  left_join(select(bc_stations, SITE_NO, PEARSEDA), 
            by = "SITE_NO") %>% 
  distinct()

bc_data$VALUE_VALEUR[bc_data$VALUE_VALEUR < -900] <- NA

write_feather(bc_data, here::here("data/bcdata.feather"))
```


```{r EC-data}
# load parameters:
bc_data <- read_feather(here::here("data/bcdata.feather"))
params <- read_csv(here::here("data/parameters.csv"))

# TODO - check params with very high CV - especially TEMPERATURE WATER vs 
# TEMPERATURE WATER (FIELD)

bc_tidy <- tidy_ec_data(bc_data, cols = "PEARSEDA") %>% 
  add_shortnames() %>% 
  filter(PARAM_SHORT_NAME %in% params$Variable_short_name, 
         year(DateTime) > 2003) %>% 
  group_by(SITE_NO, PARAM_SHORT_NAME, DateTime, Units, DetectionLimit, 
           ResultLetter, PEARSEDA) %>% 
  summarise(sd = sd(Value, na.rm = TRUE), 
            Value = mean(Value, na.rm = TRUE), 
            cv = (sd / Value) * 100,
            n = n()) %>% 
  ungroup() %>% 
  mutate(year = year(DateTime), 
         month = month(DateTime), 
         censored = make_censor(ResultLetter))

bc_site_var_summary <- bc_tidy %>% group_by(SITE_NO, PARAM_SHORT_NAME) %>% 
  summarize(min_year = min(year, na.rm = TRUE), 
            max_year = max(year, na.rm = TRUE), 
            nyears = max_year - min_year + 1,
            n_obs = n(),
            n_cens = sum(censored != "none")) %>% 
  filter(max_year >= 2013, nyears >= 10) %>% 
  ungroup()

bc_tidy <- inner_join(bc_tidy, bc_site_var_summary, 
                     by = c("SITE_NO", "PARAM_SHORT_NAME"))

```

Make nested data frames of tidy data and cleaned data - one row per parameter/site,
then a plot for each
```{r}

bc_nested <- tidyr::nest(bc_tidy, -PEARSEDA, -SITE_NO, -PARAM_SHORT_NAME, 
                         -min_year, -max_year, -nyears, -n_obs, -n_cens)

bc_nested <- bc_nested %>% mutate(
  clean_data = map(data, ~ {
    clean_wqdata(.x) %>% 
      mutate(censored = make_censor(ResultLetter), 
             month = month(Date, label = FALSE), 
             year = year(Date), 
             Time = as.numeric(Date) / 1000) %>% 
      arrange(Date)
  })
)

bc_nested <- bc_nested %>% 
  mutate(
    full_monthly = map(clean_data, ~ {
      group_by(.x, year, month) %>% 
        summarise(month_avg = mean(Value, na.rm = TRUE), 
                  n_censored = sum(censored != "none", na.rm = TRUE)) %>% 
        ungroup() %>% 
        complete(year, month) %>% 
        mutate(Date = as.Date(paste(year, month, "15", sep = "-"))) %>% 
        arrange(Date)
    }), 
    lm_plot = pmap(
      list(SITE_NO, PARAM_SHORT_NAME, clean_data), 
      function(x, y, z) {
        ggplot(data = z, mapping = aes(x = Date, y = Value)) + 
          geom_point(aes(shape = censored, colour = Outlier)) + 
          geom_line() +
          geom_smooth(method = "lm") + 
          ggtitle(paste(x, y, sep = ": "))
      }))

saveRDS(bc_nested, here("tmp/bc_nested.rds"))
```

Make some plots of all parameters at each station, save to pdf
```{r}
#plot_station_vars(fraser[fraser$SITE_NO == "BC08MF0001", ], "BC08MF0001")

sites <- unique(bc_tidy$SITE_NO)
var_plots <- map(sites, ~ plot_station_vars(filter(bc_tidy, SITE_NO == .x), .x)) %>% 
  set_names(sites)

# iwalk(var_plots, ~ ggsave(file.path(here::here("out"), paste0(.y, ".pdf")), 
#                           .x, width = 10.5, height = 8, units = "in"))
```

Fit a gam with a cyclic cubic spline smooth on month, which guarantees that the
end of December and beginning of January match up (cyclic), and a smooth trend:
```{r}

bc08fc0001_mag <- filter(bc_nested, SITE_NO == "BC08KA0007", 
                              PARAM_SHORT_NAME == "MAGNESIUM") %>% 
  pull(clean_data) %>% 
  pluck(1)

bc08fc0001_mag_gam <- mgcv::gamm(Value ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, bs = "cr", k = 20), 
                                 # correlation = corCAR1(form = ~ 1),
                                 data = bc08fc0001_mag, 
                                 # family = Gamma,
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
par(mfrow = c(1,2))
acf(resid(bc08fc0001_mag_gam$gam, main = "ACF"))
pacf(resid(bc08fc0001_mag_gam$gam, main = "pACF"))
# These look fine (I think) so don't need to add a correlation structure

## Do some checks
bc08fc0001_mag_gam$lme
intervals(bc08fc0001_mag_gam$lme, type = "var-covar")
summary(bc08fc0001_mag_gam$gam)
summary(bc08fc0001_mag_gam$lme)

par(mfrow = c(2,2))
gam.check(bc08fc0001_mag_gam$gam)
par(mfrow = c(1,1))

draw(bc08fc0001_mag_gam$gam)
# not great
```

Now let's try to plot the trend:

```{r}

# Generate a data frame to hold the predicted values
pred_data <- tibble(Date = seq(min(bc08fc0001_mag$Date, na.rm = TRUE), 
                               max(bc08fc0001_mag$Date, na.rm = TRUE), 
                               length.out = 200), 
                    Time = as.numeric(Date) / 1000, 
                    month = month(Date))

## predict trend contributions
p  <- predict(bc08fc0001_mag_gam$gam, newdata = pred_data, type = "terms", 
              se.fit = TRUE)


## combine with the predictions data, including fitted and SEs
pred_data <- mutate(pred_data, 
                    pred = p$fit[,2] + coef(bc08fc0001_mag_gam$gam)["(Intercept)"],  
                    se = p$se.fit[,2])

# Plot - note need to exp() the predicted values because they were logged
ggplot(pred_data, aes(x = Date, y = pred)) +
  geom_point(data = bc08fc0001_mag, aes(y = Value)) + 
  geom_ribbon(aes(ymin = pred - se, ymax = pred + se), fill = "grey") + 
  geom_line()

```

Find the periods of likely significant change by computing the confidence intervals
of the first derivative of the fitted values - where those confidence intervals
don't overlap zero, the rate of change is different from zero, meaning the slope 
is significantly increasing or decreasing. 

From: https://www.fromthebottomoftheheap.net/2016/03/25/additive-modeling-global-temperature-series-revisited/

```{r}

devtools::source_gist("d23ae67e653d5bfff652", filename = "simulate.gamm.R")

set.seed(10)

sims <- simulate(bc08fc0001_mag_gam, nsim = 10000, newdata = pred_data)

ci <- apply(sims, 1L, quantile, probs = c(0.025, 0.975))

pred_data <- mutate(pred_data,
                  fitted = predict(bc08fc0001_mag_gam$gam, newdata = pred_data),
                  lower_sim_ci  = ci[1, ],
                  upper_sim_ci  = ci[2, ])


ggplot(bc08fc0001_mag, aes(x = Time, y = Value)) + 
  geom_point() + 
  geom_ribbon(data = pred_data, aes(ymin = lower_sim_ci, ymax = upper_sim_ci, x = Time, y = fitted),
                 alpha = 0.8, fill = "grey") +
    geom_line(data = pred_data, aes(y = fitted, x = Time))


devtools::source_gist("ca18c9c789ef5237dbc6", filename = "derivSimulCI.R")

fd <- derivSimulCI(bc08fc0001_mag_gam, term = "Time", samples = 1000, n = 200)

CI <- apply(fd[[1]]$simulations, 1, quantile, probs = c(0.025, 0.975))

sigD <- signifD(fd[["Time"]]$deriv, fd[["Time"]]$deriv, CI[2, ], CI[1, ],
                eval = 0)

pred_data <- mutate(pred_data,
                  derivative = fd[["Time"]]$deriv[, 1], # computed first derivative
                  fdUpper = CI[2, ],                    # upper CI on first deriv
                  fdLower = CI[1, ],                    # lower CI on first deriv
                  increasing = sigD$incr,               # where is curve increasing?
                  decreasing = sigD$decr, 
                  direction = ifelse(!is.na(increasing), "increasing", 
                                     ifelse(!is.na(decreasing), "decreasing", 
                                            NA)))

# The first derivatives of the fitted trend can be used to determine where 
# values are increasing or decreasing. Using the standard error of the 
# derivative or posterior simulation we can also say where the confidence 
# interval on the derivative doesn’t include 0 — suggesting periods of 
# statistically significant change in the value.

ggplot(pred_data, aes(x = Time, y = derivative)) +
    geom_ribbon(aes(ymax = fdUpper, ymin = fdLower), alpha = 0.3, fill = "grey") +
    geom_line() +
    geom_line(aes(y = increasing), size = 1.5) +
    geom_line(aes(y = decreasing), size = 1.5) +
    ylab(expression(italic(hat(f) * "'") * (Time))) +
    xlab("Time")

# Replot original fitted line of trend, but with the period(s) identified
# by first derivative as periods of significant change

ggplot(pred_data, aes(x = Date, y = pred)) +
  geom_point(data = bc08fc0001_mag, aes(y = Value)) + 
  geom_ribbon(aes(ymin = pred - se, ymax = pred + se), fill = "grey", alpha = 0.7) + 
  geom_line() + 
  geom_line(data = pred_data[!is.na(pred_data$direction), ], 
            aes(colour = direction), 
            size = 1, lineend = "round") + 
  labs(y = "Value", colour = "Trend direction")


```

Try logging the response variable
(could also try with a different family, probably Gamma)

```{r}
bc08fc0001_mag$log_avg <- log(bc08fc0001_mag$Value)

fraser_arsenic_gam_log <- mgcv::gamm(log_avg ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, k = 20),
                                   correlation = corCAR1(),
                                 data = bc08fc0001_mag, 
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam_log$lme), main = "ACF")
pacf(resid(fraser_arsenic_gam_log$lme), main = "pACF")
# These look fine so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam_log
summary(fraser_arsenic_gam_log$gam)
gam.check(fraser_arsenic_gam_log$gam)
draw(fraser_arsenic_gam_log$gam)

```

Testing out with brms, which opens up more distributions and allows modelling 
of censored data, as well as distributional modelling of input parameters:
```{r}
library(brms)
library(bayesplot)
options(mc.cores = 4L)

# foo <- brm(bf(Value ~ s(month, bs = "cc", k = 12) + 
#          s(Time, k = 20)),
#      data = bc08fc0001_mag, 
#      family = Gamma(link = "log")) # log link to ensure not negative
# 
# pairs(foo, off_diag_args = list(size = 0.1))
# pp_check(foo)
# pp_check(foo, type = "ecdf_overlay")

foo <- brm(bf(Value| cens(censored) ~ s(month, bs = "cc", k = 12) +
                s(Time, k = 20), 
              family = gen_extreme_value(link = "log")),
           data = bc08fc0001_mag, 
           # family = lognormal(), 
           control = list(adapt_delta = 0.99))

pp_check(foo)
pp_check(foo, type = "ecdf_overlay")
summary(foo)
pairs(foo, off_diag_args = list(size = 0.1))
plot(foo, N = 2, ask = FALSE)
plot(marginal_effects(foo), ask = FALSE, points = TRUE)
```


