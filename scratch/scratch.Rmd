---
title: "Trend Exploration"
output: html_document
---

```{r setup, include=FALSE}
library(canwqdata)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(wqbc)
library(mgcv)
library(broom)
library(readr)
library(gratia) # devtools::install_github("gavinsimpson/gratia")
library(here)
library(feather)
library(purrr)
source(here::here("R/functions.R"))
dir.create(here::here("out"))
```


```{r load, eval=FALSE}
bc_stations <- wq_sites() %>% 
  filter(SITE_NO %in% wqbc::ems_ec_stations$envirodat_no)

bc_data <- wq_site_data(unique(bc_stations$SITE_NO)) %>% 
  left_join(select(bc_stations, SITE_NO, PEARSEDA), 
            by = "SITE_NO") %>% 
  distinct()

bc_data$VALUE_VALEUR[bc_data$VALUE_VALEUR < -900] <- NA

write_feather(bc_data, here::here("data/bcdata.feather"))
```


```{r EC-data}
# load parameters:
bc_data <- read_feather(here::here("data/bcdata.feather"))
params <- read_csv(here::here("data/parameters.csv"))

# TODO - check params with very high CV - especially TEMPERATURE WATER vs 
# TEMPERATURE WATER (FIELD)

bc_tidy <- tidy_ec_data(bc_data, cols = "PEARSEDA", mdl_action = "mdl") %>% 
  add_shortnames() %>% 
  filter(PARAM_SHORT_NAME %in% params$Variable_short_name, 
         year(DateTime) > 2003) %>% 
  group_by(SITE_NO, PARAM_SHORT_NAME, DateTime, Units, DetectionLimit, 
           ResultLetter, PEARSEDA) %>% 
  summarise(sd = sd(Value, na.rm = TRUE), 
            Value = mean(Value, na.rm = TRUE), 
            cv = (sd / Value) * 100,
            n = n()) %>% 
  ungroup() %>% 
  mutate(year = year(DateTime), 
         month = month(DateTime), 
         censored = make_censor(ResultLetter))

bc_site_var_summary <- bc_tidy %>% group_by(SITE_NO, PARAM_SHORT_NAME) %>% 
  summarize(min_year = min(year, na.rm = TRUE), 
            max_year = max(year, na.rm = TRUE), 
            nyears = max_year - min_year + 1,
            n_obs = n(),
            n_cens = sum(censored != "none")) %>% 
  filter(max_year >= 2013, nyears >= 10) %>% 
  ungroup()

bc_tidy <- inner_join(bc_tidy, bc_site_var_summary, 
                     by = c("SITE_NO", "PARAM_SHORT_NAME"))

# TODO: trim ends of ts when very sparse data
```

Make nested data frames of tidy data and cleaned data - one row per parameter/site,
then a plot for each
```{r}

bc_nested <- tidyr::nest(bc_tidy, -PEARSEDA, -SITE_NO, -PARAM_SHORT_NAME, 
                         -min_year, -max_year, -nyears, -n_obs, -n_cens)

bc_nested <- bc_nested %>% mutate(
  clean_data = map(data, ~ {
    clean_wqdata(.x) %>% 
      mutate(censored = make_censor(ResultLetter), 
             month = month(Date, label = FALSE), 
             year = year(Date), 
             Time = as.numeric(Date) / 1000) %>% 
      arrange(Date)
  })
)

bc_nested <- bc_nested %>% 
  mutate(
    full_monthly = map(clean_data, month_complete), 
    lm_plot = pmap(
      list(SITE_NO, PARAM_SHORT_NAME, clean_data), 
      function(x, y, z) {
        ggplot(data = z, mapping = aes(x = Date, y = Value)) + 
          geom_point(aes(shape = censored, colour = Outlier)) + 
          geom_step(aes(y = DetectionLimit)) +
          geom_smooth(method = "lm") + 
          ggtitle(paste(x, y, sep = ": "))
      }))

saveRDS(bc_nested, here("tmp/bc_nested.rds"))
```

Make some plots of all parameters at each station, save to pdf
```{r}
#plot_station_vars(fraser[fraser$SITE_NO == "BC08MF0001", ], "BC08MF0001")

sites <- unique(bc_tidy$SITE_NO)
var_plots <- map(sites, ~ plot_station_vars(filter(bc_tidy, SITE_NO == .x), .x)) %>% 
  set_names(sites)

# iwalk(var_plots, ~ ggsave(file.path(here::here("out"), paste0(.y, ".pdf")), 
#                           .x, width = 10.5, height = 8, units = "in"))
```

Fit a gam with a cyclic cubic spline smooth on month, which guarantees that the
end of December and beginning of January match up (cyclic), and a smooth trend:
```{r}

bc08fc0001_mag <- filter(bc_nested, SITE_NO == "BC08KA0007", 
                              PARAM_SHORT_NAME == "MAGNESIUM") %>% 
  pull(full_monthly) %>% 
  pluck(1) %>% 
  mutate(time = as.numeric(Date) / 1000)

## Checking time series properties
# library(astsa)
# mag_ts <- ts(bc08fc0001_mag$month_avg, start = c(2004, 1), frequency = 12)
# plot(mag_ts)
# # trend, so difference to remove it
# dl_mag_ts <- diff(mag_ts)
# plot(dl_mag_ts)
# par(mfrow = c(2, 1))
# acf(dl_mag_ts, lag.max = 60, na.action = na.pass)
# pacf(dl_mag_ts, lag.max = 60, na.action = na.pass, ylim = c(-2, 2))
# # ACF trails off at seasonal intervals and within seasons
# # pACF is weird, but looks like it trails off seasonally, and tails off within seasons
# p1q0P1Q0 <- sarima(mag_ts, p = 0, d = 1, q = 0, P = 1, D = 1, Q = 0, S = 12)
# p1q0P1Q1 <- sarima(mag_ts, p = 1, d = 1, q = 0, P = 1, D = 1, Q = 1, S = 12)
# p1q1P1Q1 <- sarima(mag_ts, p = 1, d = 1, q = 1, P = 1, D = 1, Q = 1, S = 12)
# p1q1P0Q1 <- sarima(mag_ts, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)
# data.frame(p1q0P1Q0$AICc, p1q0P1Q1$AICc, p1q1P1Q1$AICc, p1q1P0Q1$AICc)
# # p1q1P1Q1 wins: SARIMA(1,1,1)x(1,1,1)12 but diagnostics still not great.


# use corARMA with p =1 (seasonal should already by taken car of by seasonal smooth on month)
bc08fc0001_mag_gam <- mgcv::gamm(
  month_avg ~ s(month, bs = "cc", k = 12) + 
    s(time), 
  correlation = corARMA(p = 1),
  data = bc08fc0001_mag, 
  family = Gamma(link = "log"),
  method = "REML"
)

# Check the residuals for evidence of autocorrelation:
par(mfrow = c(1,2))
acf(resid(bc08fc0001_mag_gam$gam), lag.max = 60, main = "ACF")
pacf(resid(bc08fc0001_mag_gam$gam), lag.max = 60, main = "pACF")
# These look fine (I think) so don't need to add a correlation structure

## Do some checks
bc08fc0001_mag_gam$lme
intervals(bc08fc0001_mag_gam$lme, type = "var-covar")
summary(bc08fc0001_mag_gam$gam)
summary(bc08fc0001_mag_gam$lme)

par(mfrow = c(2,2))
gam.check(bc08fc0001_mag_gam$gam)
par(mfrow = c(1,1))

draw(bc08fc0001_mag_gam$gam)
```

Now let's try to plot the trend:

```{r}

# Generate a data frame to hold the predicted values
pred_data <- tibble(Date = seq(min(bc08fc0001_mag$Date, na.rm = TRUE), 
                               max(bc08fc0001_mag$Date, na.rm = TRUE), 
                               length.out = 200), 
                    time = as.numeric(Date) / 1000, 
                    month = month(Date))

## predict trend contributions
p  <- predict(bc08fc0001_mag_gam$gam, newdata = pred_data, type = "terms", 
              se.fit = TRUE)


## combine with the predictions data, including fitted and SEs
pred_data <- mutate(pred_data, 
                    pred = p$fit[,2] + coef(bc08fc0001_mag_gam$gam)["(Intercept)"],  
                    se = p$se.fit[,2])

# Plot - note need to exp() the predicted values because fit on log scale
ggplot(pred_data, aes(x = Date, y = exp(pred))) +
  geom_ribbon(aes(ymin = exp(pred - se), ymax = exp(pred + se)), 
              fill = "skyblue2", alpha = 0.3) + 
  geom_point(data = bc08fc0001_mag, aes(y = month_avg)) + 
  geom_line(colour = "blue")

```

Find the periods of likely significant change by computing the confidence intervals
of the first derivative of the fitted values - where those confidence intervals
don't overlap zero, the rate of change is different from zero, meaning the slope 
is significantly increasing or decreasing. 

From: https://www.fromthebottomoftheheap.net/2016/03/25/additive-modeling-global-temperature-series-revisited/

```{r}
set.seed(10)

sims <- simulate(bc08fc0001_mag_gam, nsim = 10000, newdata = pred_data)

ci <- apply(sims, 1L, quantile, probs = c(0.025, 0.975))

pred_data <- mutate(pred_data,
                  fitted = predict(bc08fc0001_mag_gam$gam, newdata = pred_data),
                  lower_sim_ci  = ci[1, ],
                  upper_sim_ci  = ci[2, ])

ggplot(bc08fc0001_mag, aes(x = time)) + 
  geom_point(aes(y = month_avg)) + 
  geom_ribbon(data = pred_data, 
              aes(ymin = exp(lower_sim_ci), ymax = exp(upper_sim_ci)),
              alpha = 0.8, fill = "grey") +
  geom_line(data = pred_data, aes(y = exp(fitted)))

fd <- fderiv(bc08fc0001_mag_gam, term = "time", n = 200)

fd_ci <- confint(fd, parm = "time", type = "simultaneous", nsim = 10000)

# Determine areas of significant increase and decrease
fd_ci <- mutate(fd_ci,
                incr = est > 0 & lower > 0,
                decr = est < 0 & upper < 0)

# Calculate simultaneous 95% confidence intervals on s(time)
pred_ci <- confint(bc08fc0001_mag_gam, parm = "time", type = "simultaneous", 
                   nsim = 10000)

## Add the first derivatives and their ci's to the predicted data
pred_data <- bind_cols(
  pred_data, 
  transmute(fd_ci,
            fderiv = est, # computed first derivative
            fd_upper = upper,  # upper CI on first deriv
            fd_lower = lower,  # lower CI on first deriv
            # where is curve increasing?
            increasing = ifelse(incr, fderiv, NA_real_),
            # where is curve decreasing?
            decreasing = ifelse(decr, fderiv, NA_real_),
            direction = factor(ifelse(incr, "increasing", 
                               ifelse(decr, "decreasing", 
                                      NA)), 
                               levels = c("increasing", "decreasing"))), 
  transmute(pred_ci, 
            pred_time = est,
            time_sim_upper = upper, 
            time_sim_lower = lower)
  )

# The first derivatives of the fitted trend can be used to determine where 
# values are increasing or decreasing. Using the standard error of the 
# derivative or posterior simulation we can also say where the confidence 
# interval on the derivative doesn't include 0 - suggesting periods of 
# statistically significant change in the value.

ggplot(pred_data, aes(x = time, y = fderiv)) +
    geom_ribbon(aes(ymax = fd_upper, ymin = fd_lower), alpha = 0.3, fill = "grey") +
    geom_line() +
    geom_line(aes(y = increasing), size = 1.5) +
    geom_line(aes(y = decreasing), size = 1.5) +
    labs(
      title = "First derivative of fitted trend plus simultaneous confidence intervals",
      caption = "Where CI's don't cross zero it denotes a period where the trend is differnt from zero.",
      y = expression(italic(hat(f) * "'") * (time)), 
      x = "Time"
      )

# Replot original fitted line of trend, but with the period(s) identified
# by first derivative as periods of significant change

ggplot(pred_data, aes(x = Date, y = exp(pred_time))) +
  geom_point(data = bc08fc0001_mag, aes(y = month_avg)) + 
  geom_ribbon(aes(ymin = exp(time_sim_lower), ymax = exp(time_sim_upper)), 
              fill = "grey", alpha = 0.7) + 
  geom_line() + 
  geom_line(data = pred_data[!is.na(pred_data$direction), ], 
            aes(colour = direction), 
            size = 1, lineend = "round") + 
  scale_color_manual(values = c("skyblue2", "orange"), drop = FALSE) + 
  labs(
    title = "Estimated smooth trend of concentration of Magnesium",
    caption = "Periods where the trend is estimated to be significantly different 
from zero are shown in blue (increasing) or orange (decreasing)",
    y = "Value", colour = "Trend direction")


```

Try logging the response variable
(could also try with a different family, probably Gamma)

```{r}
bc08fc0001_mag$log_avg <- log(bc08fc0001_mag$Value)

fraser_arsenic_gam_log <- mgcv::gamm(log_avg ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, k = 20),
                                   correlation = corCAR1(),
                                 data = bc08fc0001_mag, 
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam_log$lme), main = "ACF")
pacf(resid(fraser_arsenic_gam_log$lme), main = "pACF")
# These look fine so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam_log
summary(fraser_arsenic_gam_log$gam)
gam.check(fraser_arsenic_gam_log$gam)
draw(fraser_arsenic_gam_log$gam)

```

Testing out with brms, which opens up more distributions and allows modelling 
of censored data, as well as distributional modelling of input parameters:
```{r}
library(brms)
library(bayesplot)
options(mc.cores = 4L)

# foo <- brm(bf(Value ~ s(month, bs = "cc", k = 12) + 
#          s(Time, k = 20)),
#      data = bc08fc0001_mag, 
#      family = Gamma(link = "log")) # log link to ensure not negative
# 
# pairs(foo, off_diag_args = list(size = 0.1))
# pp_check(foo)
# pp_check(foo, type = "ecdf_overlay")

foo <- brm(bf(Value | cens(censored) ~ s(month, bs = "cc", k = 12) +
                s(Time, k = 20)),
           data = bc08fc0001_mag, 
           family = Gamma(link = "log"))

pp_check(foo)
pp_check(foo, type = "ecdf_overlay")
summary(foo)
pairs(foo, off_diag_args = list(size = 0.1))
plot(foo, N = 2, ask = FALSE)
plot(marginal_effects(foo), ask = FALSE, points = TRUE)
```


