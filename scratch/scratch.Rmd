---
title: "Trend Exploration"
output: html_notebook
---

```{r setup, include=FALSE}
library(canwqdata)
library(dplyr)
library(ggplot2)
library(lubridate)
library(wqbc)
library(mgcv)
library(broom)
library(readr)
library(schoenberg) # devtools::install_github('gavinsimpson/schoenberg')
library(here)
library(feather)
library(purrr)
source(here::here("R/functions.R"))
dir.create(here::here("out"))
```


```{r load, eval=FALSE}
bc_stations <- wq_sites() %>% 
  filter(PROV_TERR == "BC")

bc_data <- wq_site_data(unique(bc_stations$SITE_NO)) %>% 
  left_join(select(bc_stations, SITE_NO, PEARSEDA), 
            by = "SITE_NO")

bc_data$VALUE_VALEUR[bc_data$VALUE_VALEUR < -999] <- NA

write_feather(bc_data, here::here("data/bcdata.feather"))
```


```{r EC-data}
# load parameters:
bc_data <- read_feather(here::here("data/bcdata.feather"))
params <- read_csv(here::here("data/parameters.csv"))

bc_tidy <- tidy_ec_data(bc_data, cols = "PEARSEDA") %>% 
  add_shortnames() %>% 
  filter(PARAM_SHORT_NAME %in% params$Variable_short_name, 
         year(DateTime) >= 2003) %>% 
  group_by(SITE_NO, PARAM_SHORT_NAME, DateTime, Units, DetectionLimit, 
           ResultLetter, PEARSEDA) %>% 
  summarise(Value = mean(Value, na.rm = TRUE), 
            n = n()) %>% 
  ungroup() %>% 
  mutate(year = year(DateTime), 
         month = month(DateTime), 
         censored = make_censor(ResultLetter))

bc_site_var_summary <- bc_tidy %>% group_by(SITE_NO, PARAM_SHORT_NAME) %>% 
  summarize(min_year = min(year, na.rm = TRUE), 
            max_year = max(year, na.rm = TRUE), 
            nyears = max_year - min_year,
            n_obs = n(),
            n_cens = sum(censored != "none")) %>% 
  filter(max_year >= 2013, nyears >= 10) %>% 
  ungroup()

bc_tidy <- semi_join(bc_tidy, bc_site_var_summary, 
                     by = c("SITE_NO", "PARAM_SHORT_NAME"))

fraser <- filter(bc_tidy, PEARSEDA == "FRASER-LOWER MAINLAND")


```

```{r}
plot_station_vars(fraser, "BC08MF0001")
sites <- unique(bc_tidy$SITE_NO)
var_plots <- map(sites, ~ plot_station_vars(bc_tidy, .x)) %>% 
  set_names(sites)


iwalk(var_plots, ~ ggsave(file.path(here::here("out"), paste0(.y, ".pdf")), 
                          .x, width = 10.5, height = 8, units = "in"))
```


```{r}
# foo <- standardize_wqdata(fraser_tidy)

# Try a trend test with Fraser River at Hope
fraser_hope_arsenic <- filter(fraser, 
                              SITE_NO == "BC08MF0001",
                              PARAM_SHORT_NAME == "ARSENIC TOTAL") %>% 
  ungroup()

## Only keep data from 2003 on, Aggregate by month, encode a numeric 'Time' 
# variable from Date
# fraser_hope_arsenic <- fraser_hope_arsenic %>% 
#   mutate(month = month(DateTime, label = FALSE), 
#          year = year(DateTime)) %>% 
#   filter(year >= 2003) %>% 
#   group_by(PARAM_SHORT_NAME, month, year, censored) %>% 
#   summarise(month_avg = mean(avg_val, na.rm = TRUE)) %>% 
#   mutate(Date = as.Date(paste(year, month, "15", sep = "-")))


# Make it into a complete time series:
# month_df <- data.frame(Date = seq(min(fraser_hope_arsenic$Date), 
#                                   max(fraser_hope_arsenic$Date), 
#                                   by = "month"))

fraser_hope_arsenic <- fraser_hope_arsenic %>% #right_join(fraser_hope_arsenic, month_df) %>% 
  ungroup() %>% 
  mutate(Date = as.Date(DateTime),
    month = month(Date, label = FALSE), 
         year = year(Date), 
         Time = as.numeric(Date) / 1000) %>% 
  arrange(Date)

ggplot(fraser_hope_arsenic, aes(x = Date, y = Value)) + 
  geom_point(aes(colour = censored)) + 
  geom_line() +
  geom_smooth(method = "lm")
```

Fit a gamm with a cyclic cubic spline smooth on month, which guarantees that the
end of December and beginning of January match up (cyclic), and a smooth trend:
```{r}
fraser_arsenic_gam <- mgcv::gamm(Value ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, k = 20), 
                                 # correlation = corARMA(form = ~ 1, p = 2),
                                 data = fraser_hope_arsenic, 
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam$lme, main = "ACF"))
pacf(resid(fraser_arsenic_gam$lme, main = "pACF"))
# These look fine (I think) so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam
summary(fraser_arsenic_gam$gam)
gam.check(fraser_arsenic_gam$gam)
draw(fraser_arsenic_gam$gam)
# not great
```

Those checks on the model fit aren't great - try logging the response variable
(could also try with a different family, probably Gamma)

```{r}
fraser_hope_arsenic$log_avg <- log(fraser_hope_arsenic$month_avg)

fraser_arsenic_gam_log <- mgcv::gamm(log_avg ~ s(month, bs = "cc", k = 12) + 
                                   s(Time, k = 20),
                                 data = fraser_hope_arsenic, 
                                 method = "REML")

# Check the residuals for evidence of autocorrelation:
acf(resid(fraser_arsenic_gam_log$lme), main = "ACF")
pacf(resid(fraser_arsenic_gam_log$lme), main = "pACF")
# These look fine so don't need to add a correlation structure

## Do some checks
fraser_arsenic_gam_log
summary(fraser_arsenic_gam_log$gam)
gam.check(fraser_arsenic_gam_log$gam)
draw(fraser_arsenic_gam_log$gam)

```

Now let's try to plot the trend:

```{r}

# Generate a data frame to hold the predicted values
pred_data <- tibble(Date = seq(min(fraser_hope_arsenic$Date, na.rm = TRUE), 
                               max(fraser_hope_arsenic$Date, na.rm = TRUE), 
                               length.out = 200), 
                    Time = as.numeric(Date) / 1000, 
                    month = month(Date))

## predict trend contributions
p  <- predict(fraser_arsenic_gam_log$gam, newdata = pred_data, type = "terms", 
              se.fit = TRUE)


## combine with the predictions data, including fitted and SEs
pred_data <- mutate(pred_data, 
                    pred = p$fit[,2] + coef(fraser_arsenic_gam_log$gam)["(Intercept)"],  
                    se = p$se.fit[,2])

# Plot - note need to exp() the predicted values because they were logged
ggplot(pred_data, aes(x = Date, y = exp(pred))) +
  geom_point(data = fraser_hope_arsenic, aes(y = month_avg)) + 
  geom_ribbon(aes(ymin = exp(pred - se), ymax = exp(pred + se)), fill = "grey") + 
  geom_line()

```


Testing out USGS seaken method:
```{r}
library(smwrStats)
library(smwrGraphs)

fraser_hope_arsenic_sk <- seaken(fraser_hope_arsenic$month_avg)
fraser_hope_arsenic_sk
# seriesPlot(fraser_hope_arsenic_sk)


```


EnvStats:

```{r}
library(EnvStats)

ks_fraser <- kendallSeasonalTrendTest(month_avg ~ month + year, 
                                      data = fraser_hope_arsenic)

ks_fraser
# In order to account for serial autocorrelation there must be the same number
# of observations in each year (12) - could either pad start and end years 
# with NAs or truncate.
```

Plot the Seasonal Kendall results: need to rescale the slope and intercept 
because they are based on time starting at time zero, and incrementing up by month.
The date scale starts at `as.numeric(min(fraser_hope_arsenic$Date))` and increments
by one per day

```{r}
slope <- ks_fraser$estimate["slope"] / (365 / 12)
# The overall intercept is the median of all seasonal intercepts... doesn't seem
# right
int <- ks_fraser$estimate["intercept"] - 
  (slope * min(as.numeric(fraser_hope_arsenic$Date)))
  

ggplot(fraser_hope_arsenic, aes(x = Date, y = month_avg)) + 
  geom_point() + 
  geom_abline(slope = slope, 
              intercept = int)

# plotting on a sequence scale (as the MK test is done on) produces the same 
# figure, so I think I am transforming the slope and intercept correctly.
plot(fraser_hope_arsenic$month_avg ~ seq_len(nrow(fraser_hope_arsenic)))
abline(ks_fraser$estimate["intercept"], ks_fraser$estimate["slope"])

```

